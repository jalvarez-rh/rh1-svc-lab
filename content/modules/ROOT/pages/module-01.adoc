[[operations-security]]

= Operations and Security

**Operations and Security Sovereignty** means ensuring organizations can operate their technology independently and autonomously, with full control over their data, infrastructure, and security postures.

[[multi-env-clusters]]

== Multi-Environment Red Hat OpenShift Cluster and Virtual Machine Creation and Management

Cluster creation and management marks the essential first step in building a truly sovereign cloud environment. By owning the full lifecycle of your clusters—from initial deployment to ongoing operations—your organization maintains direct control over infrastructure, security policies, and data locality. This foundational capability ensures that critical workloads and data remain within your jurisdiction, enabling you to adapt rapidly to regulatory changes, respond to evolving business needs, and operate independently of external providers. Flexible cluster and virtual machine management empowers you to design cloud environments tailored specifically to your sovereignty requirements—ensuring resilience, autonomy, and full compliance from the ground up.

[[hub-cluster-overview]]
=== RHACM Management Cluster Overview

The hub cluster in Red Hat Advanced Cluster Management (RHACM) serves as the central control plane of your sovereign cloud environment for managing your entire fleet of OpenShift clusters. It provides a single point of authority and unified interface for cluster lifecycle management, policy enforcement, application deployment, and observability across multiple environments.

=== Create and Manage Kubernetes Clusters

Red Hat Advanced Cluster Management for Kubernetes (RHACM) simplifies the deployment and management of new clusters. While Red Hat OpenShift offers easy deployment methods like IPI and the Assisted Installer, RHACM takes it a step further, allowing you to deploy new clusters with just a few clicks using the cluster creation wizard.

From the Clusters screen, you can quickly see how simple it is to deploy a new cluster.

*Procedure*

[start=1]
. In the top left of the screen, click the drop down menu and select *Fleet Management*
. Next, click on the *Create cluster* button in the center of the screen:

image::103-create-cluster.png[link=self, window=blank, width=100%, Create Cluster]

NOTE: You’ll notice that the only option already highlighted is Red Hat OpenShift Virtualization, indicating that your credentials are saved for the purpose of speeding up this lab. You will use this to deploy the new cluster, but feel free to explore the window to see other available cluster types.

[start=3]
. Click on the Red Hat OpenShift Virtualization button. You will see one options for the control plane type: *Hosted*

image::104-aws-credentials.png[link=self, window=blank, width=100%, AWS Credentials]

[start=4]
. Click on the *hosted* option.
. Leave the infrastructure Provider Credentials as *hcp* 
. Name your cluster *hcp-emea*, and verify that is on the *clusters* space for the hosted cluster namespace
. Select *managed* for the Cluster Set
. Next, select the release image *OpenShift 4.19.21*. 
. Select *single replica* for both the Controller availability policy and the Infrastructure availability policy. 

[NOTE]
====
Single replica means components are not expected to be resilient to problems across most fault boundaries associated with high availability. This usually means running critical workloads with just 1 replica and with toleration of full disruption of the component.
====

[start=10]
. Create a label called *location=emea* - we will use this later on for other exercises.
. Click on *Next* to continue.

image::105-create-cluster-details.png[link=self, window=blank, width=100%, Create Cluster Details]

On the next screen you can customize the name for the node pools, the amount of pool replicas, cores and memory, enter the name *hcp-emea* for the Node pool name and leave the other details as default.

[start=11]
. Click on *Next* to proceed.

image::106-create-cluster-nodepools.png[link=self, window=blank, width=100%, Create Cluster NodePools]

The next screen allows you to configure storage mappings type to use and it's associated variables, we will leave these as default as these have already been configured on the backend.

[start=12]
. Click on *Next* to proceed.

image::107-create-cluster-networking.png[link=self, window=blank, width=100%, Create Cluster Networking]

[start=13]
. Click *Next* on each screen to proceed to the final *Review and Create* screen. You will see a description of the cluster you are creating.
. Click the blue *Create* button to start the deployment process.

image::108-create-cluster-summary-create.png[link=self, window=blank, width=100%, Create Cluster Summary Page]

[start=15]
. Sit back and watch RHACM do it's thing. You should see the Cluster creation process starting in the UI.

image::03-cluster-creation.png[link=self, window=blank, width=100%, View New Cluster]

NOTE: Deploying this Hosted Control Plane (HCP) cluster will take about 5 to 10 minutes. You’ll continue the lab while waiting for this cluster to provision, we will use it later on.

[[label-an-existing-cluster]]



[[multi-env-workloads]]

== Multi-environment Workload Creation, Management, and Observability

Sovereign cloud operations require that organizations retain full control over where and how their applications run. RHACM empowers you to manage all workloads—VMs, containers, and AI/ML—centrally, ensuring policy-driven governance and compliance across multiple environments.

[[create-manage-vms]]

== Create and Manage Virtual Machines Using the RHACM Console

Red Hat Advanced Cluster Management (RHACM) with OpenShift Virtualization delivers centralized, policy-driven control—labeling clusters/VMs (e.g., location=emea/us), enforcing placement rules, and automating compliance to keep sovereign workloads in approved geographies via a unified console. In traditional environments, administrators often struggle with context switching—jumping between virtualization hypervisors to manage legacy workloads and Kubernetes consoles to manage modern applications. This fragmentation slows down operations and complicates governance. This single-pane-of-glass approach for VMs alongside Kubernetes resources simplifies operations, ensures auditability, and applies consistent sovereign controls to legacy and cloud-native workloads, mirroring real-world architect workflows for scalable, secure multi-cluster solutions.

By the end of this exercise, you will:

* Navigate the RHACM Search interface to locate Virtual Machine resources across the entire fleet.
* Provision a new Virtual Machine using the RHACM Creation Wizard.
* Perform Day 2 operations (Stop, Start, Migrate) directly from the multi-cluster view.

=== Deploy Virtual Machines Using the RHACM Console

*Procedure*

[start=1]
. On the left switcher use the drop down to select *Fleet Virtualization*
+
image::08-dropdown-menu.png[link=self, window=blank, width=100%, Create VM]

. Click the *Create* dropdown select *From template*
+
image::09-create-menu.png[link=self, window=blank, width=100%, Create VM]

. Under default template select *Red Hat Enterprise Linux 9 VM*
+
image::10-template-menu.png[link=self, window=blank, width=100%, Create VM]
+
NOTE: Notice all of the avaiable templates to use, Windows VMs are fully supported as well.

. Under Virtual machine name, enter the following and leave all other options as default (Disk size, Disk Source):
+
[source,sh]
----
rhel9-lab-vm
----
+
[NOTE]
==== 
Please notice all of the available options to configure the VM with, these will change depending on your operating system
====
+
. Click *Quick create Virtual Machine* and watch for progress, this should take a couple of minutes to complete.

. Once completed you will see the live console as well as stats about your VM.

image::12-vm-finish.png[link=self, window=blank, width=100%, Create VM]

Feel free to explore the available management screens for VMs, from Metrics to Console to Snapshots and more! 

[TIP]:
.UI or GitOps—The Choice is Yours!
====
While this lab demonstrates the graphical capabilities of the RHACM console, Virtual Machines in OpenShift are fundamentally Kubernetes objects. You can define your VMs as code using YAML and deploy them using Red Hat OpenShift GitOps (ArgoCD). RHACM supports both workflows: use the UI for quick administration and discovery, or use GitOps for a fully declarative, audit-ready production pipeline. In sovereign cloud environments, GitOps workflows provide version-controlled infrastructure that meets compliance requirements and audit trails for regulatory demonstrations.
====

[[container-workloads]]
=== Deploy and Manage Container Workloads via OpenShift GitOps

Red Hat® OpenShift® GitOps, powered by Argo CD and integrated with Red Hat Advanced Cluster Management (RHACM), makes application delivery faster, more secure, and consistent by using Git as the single source of truth to automatically handle declarative deployments and configurations across multiple clusters.

In sovereign cloud environments—where strict data residency, regulatory compliance, and operational control are essential—the optional pull model is ideal. Remote clusters securely pull updates from Git on their own, without needing inbound connections from a central hub, which strengthens security, minimizes network risks, and keeps workloads strictly within approved geographic or jurisdictional boundaries.

For this exercise, you’ll use the simpler push model (with Argo CD already deployed in your environment and ready for RHACM configuration), giving you a centralized, easy-to-manage approach that still demonstrates how GitOps delivers scalable operations while fully supporting sovereign cloud requirements in hybrid and multi-cloud setups.

[[deploy-applications-using-openshift-gitops]]

== Deploy Applications Using OpenShift GitOps

In this next step, you'll use Red Hat® OpenShift® GitOps with Argo CD to declaratively deploy the Skupper Patient Demo Application to both your local and hosted control plane (HCP) clusters—showcasing automated, Git-driven multi-cluster consistency while supporting sovereign cloud requirements by keeping patient data securely within approved geographic boundaries.

*Procedure*

[start=1]
. From the **Fleet Management** tab, navigate to *Applications* from the left side menu.
. Click *Create application, select ArgoCD AppicationSet-Push Model*.
. Enter the following information:
* *Name:* Enter the following:
+
[source,sh]
----
skupper-patient-demo
----
. Under the Argo server select *Add Argo Server* 
+
image::13-argo.png[link=self, window=blank, width=100%, ArgoCD Create]
+
. Enter the following information:
* *Name:* Enter the following:
+
[source,sh]
----
openshift-gitops
----
+
* *Namespace:* Enter the following:openshift-gitops
* *ClusterSet:* managed

image::03-argoconfig.png[link=self, window=blank, width=100%, ArgoCD Config]

[[deploy-applications-using-openshift-gitops]]

== Deploy Applications Using OpenShift GitOps

In this next step, you'll use Red Hat® OpenShift® GitOps with Argo CD to declaratively deploy the Skupper Patient Demo Application to both your local and hosted control plane (HCP) clusters—showcasing automated, Git-driven multi-cluster consistency while supporting sovereign cloud requirements by keeping patient data securely within approved geographic boundaries.

*Procedure*

. Navigate to *Applications* from the left side menu.
. Click *Create application*, select *ArgoCD AppicationSet-Push Model*.
. Enter the following information:
* *Name:* skupper-patient-demo-world
* *Argo Server:* openshift-gitops
. Click *NEXT*
. Click *Add* then *NEXT*

image::03-app-gitops.png[link=self, window=blank, width=100%, App GitOps]

[start=5]
. Under repository types, select the GIT repository
. enter the URL:
+
[source,sh,subs="attributes",role=execute]
----
https://github.com/mfosterrox/demo-applications.git
----
. Set Revision: main
. Set Path:
+
[source,sh,subs="attributes",role=execute]
----
skupper-demo
----
. Set Destination:
+
[source,sh,subs="attributes",role=execute]
----
patient-portal
----
. Then click *NEXT*

image::03-app-gitops-2.png[link=self, window=blank, width=100%, App ACM GitOps]

[start=11]
. Under *Sync Policy* uncheck *Automaticaly sync when cluster state changes* and check *Replace resources instead of applying changes from the source repository* 

image::03-app-gitops-3.png[link=self, window=blank, width=100%, App ACM GitOps]

NOTE: These changes are only required as you will be modifying the application YAML on RHACM and you don't want it to sync to a Git Repo, you normaly wouldn't uncheck these in a real production enviroment.

[start=12]
. Under *Placement* for application deployment, verify that *New Placement* is selected.
* *Cluster set:* Select nothing. This will deploy the application to all clusters.
. Under *Label expressions* click *add label* and select the following
* *Label:* location
* *Operator:* equals any off
* *Values:* us, emea.
. Make sure that *Set a limit on the number of clusters selected* is checked 
. Set the number to 2
* *Values:* us, on-premise
+
. Uncheck "Set a limit on the number of clusters selected"

[IMPORTANT] 
====
Ensure Cluster set is blank and both label values to be present in the placement rule, otherwise you will have issues in the next steps.
====

image::03-app-placement-us.png[link=self, window=blank, width=100%, ACM App Placement US]

[start=14]
. Verify all of the information is correct and click *Submit*.

NOTE: It will take a few minutes to deploy the application, during this time you might see the Cluster / Application have a red X, this is expected and normal, please allow 5 minutes before checking logs.

NOTE: We will use these applications in other modules of the lab hence why we have you deploy it twice in two different locations. 
NOTE: It will take a few minutes to deploy the application, during this time you might see the Cluster / Application have a red X, this is expected and normal.


[start=15]
. Navigate back to the application view, find the filter and select *Application Set*
. Find the *skupper-patient-demo-world* app 
. Click on the *Topology Tab* to view and verify that *all of the circles are green*.

image::03-application-topology-git.png[link=self, window=blank, width=100%, Application Topology]

[start=16]
. Under the topology view, Select the *Route* and click on the *Launch Route URL*

NOTE: This will take you to the Front end for the Patient Portal application, which is now running in both the *aws-us* cluster and the *hcp-emea* Hosted Controlled Plane (HCP) CLuster.

IMPORTANT: If you get a "Application is not available" page change the URL to use http:// and that should fix the issue. 

image::03-application-route-git.png[link=self, window=blank, width=100%, Application Route]

**Congratulations! **

image::https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExdjl5YTd4M2o1MWZ4ZjBmMjM3cmJyMHYzZTFsbHB4ZDg0MXN1dzJvcCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/ely3apij36BJhoZ234/giphy.gif[Celebration, width=400, align="center"]

