= Operations & Security

**Operations and Security Sovereignty** means ensuring organizations can operate their technology independently and autonomously, with full control over their data, infrastructure, and security postures.

[[multi-env-clusters]]

== Multi-environment OCP Cluster & Virtual Machine Creation & Management.

Cluster creation and management marks the essential first step in building a truly sovereign cloud environment. By owning the full lifecycle of your clusters—from initial deployment to ongoing operations—your organization maintains direct control over infrastructure, security policies, and data locality. This foundational capability ensures that critical workloads and data remain within your jurisdiction, enabling you to adapt rapidly to regulatory changes, respond to evolving business needs, and operate independently of external providers. Flexible cluster management empowers you to design cloud environments tailored specifically to your sovereignty requirements—ensuring resilience, autonomy, and full compliance from the ground up.

[[hub-cluster-overview]]
=== RHACM Management Cluster Overview

The hub cluster in Red Hat Advanced Cluster Management (RHACM) serves as the central control plane for managing your entire fleet of OpenShift clusters. It provides a unified interface for cluster lifecycle management, policy enforcement, application deployment, and observability across multiple environments.

[[managed-clusters]]
=== Managed Clusters Overview

Managed clusters are OpenShift clusters that are registered with and managed by the RHACM hub cluster. These clusters can span multiple cloud providers, on-premises data centers, or edge locations, providing a consistent management experience regardless of where your workloads run.

[[cluster-management]]
=== Cluster Management Operations

[[cluster-observability]]
=== Cluster Observability

Cluster observability in RHACM provides comprehensive insights into the health, performance, and status of your managed clusters. This includes metrics, logs, and events aggregated from across your fleet, enabling proactive monitoring and troubleshooting.

=== Create and Manage Kubernetes Clusters

Red Hat Advanced Cluster Management for Kubernetes (RHACM) simplifies the deployment and management of additional clusters. While Red Hat OpenShift offers easy deployment methods like IPI and the Assisted Installer, RHACM takes it a step further, allowing you to deploy new clusters with just a few clicks using the cluster creation wizard.

From the Clusters screen, you can quickly see how simple it is to deploy a new cluster.

*Procedure*

[start=1]
. In the top left of the screen, click the drop down menu and select *Fleet Management*
. Next, click on the *Create cluster* button in the center of the screen:

image::103-create-cluster.png[link=self, window=blank, width=100%, Create Cluster]

NOTE: You’ll notice that the only option already highlighted is Red Hat OpenShift Virtualization, indicating that your credentials are saved. You will use this to deploy the new cluster, but feel free to explore the window to see other available cluster types.

[start=3]
. Click on the Red Hat OpenShift Virtualization button. You will see one options for the control plane type: *Hosted*

image::104-aws-credentials.png[link=self, window=blank, width=100%, AWS Credentials]

[start=4]
. Click on the *hosted* option.
. Leave the infrastructure Provider Credentials as *hcp* 
. Name your cluster *hcp-emea*, and verify that is on the *clusters* space for the hosted cluster namespace
. Select *default* for the Cluster Set
. Next, select the release image *OpenShift 4.20.8*. 
. Select *single replica* for both the Controller availability policy and the Infrastructure availability policy. 

[TIP]
====
Single replica means components are not expected to be resilient to problems across most fault boundaries associated with high availability. This usually means running critical workloads with just 1 replica and with toleration of full disruption of the component.
====

[start=10]
. Create a label called *location=emea* - we will use this later on for other exercises.
. Click on *Next* to continue.

image::105-create-cluster-details.png[link=self, window=blank, width=100%, Create Cluster Details]

On the next screen you can customize the name for the node pools, the amount of pool replicas, cores and memory, enter the name *hcp-emea* for the Node pool name and leave the other details as default.

[start=11]
. Click on *Next* to proceed.

image::106-create-cluster-nodepools.png[link=self, window=blank, width=100%, Create Cluster NodePools]

The next screen allows you to configure storage mappings type to use and it's associated variables, we will leave these as default as these have already been configured on the backend.

[start=12]
. Click on *Next* to proceed.

image::107-create-cluster-networking.png[link=self, window=blank, width=100%, Create Cluster Networking]

[start=13]
. Click *Next* on each screen to proceed to the final *Review and Create* screen.

You will see a description of the cluster you are creating.

[start=14]
. Click the blue *Create* button to start the deployment process.

image::108-create-cluster-summary-create.png[link=self, window=blank, width=100%, Create Cluster Summary Page]

[start=15]
. Sit back and watch RHACM do it's thing. You should see the Cluster creation process starting in the UI.

image::03-cluster-creation.png[link=self, window=blank, width=100%, View New Cluster]

NOTE: Deploying this Hosted Control Plane (HCP) cluster will take about 5 to 10 minutes. You’ll continue the lab while waiting for this cluster to provision, we will use it later on.

[[multi-env-workloads]]

== Multi-environment Workload Creation, Management, and Observability

Workload management across multiple environments enables organizations to deploy and manage applications consistently while maintaining sovereignty requirements. This includes support for virtual machines, container workloads, and AI/ML workloads, all managed through a unified interface.

[[vm-workloads]]
[[create-manage-vms]]

== Create and Manage Virtual Machine

Red Hat Advanced Cluster Management (RHACM) with OpenShift Virtualization delivers centralized, policy-driven control—labeling clusters/VMs (e.g., location=emea), enforcing placement rules, and automating compliance to keep sovereign workloads in approved geographies via a unified console. In traditional environments, administrators often struggle with context switching—jumping between virtualization hypervisors to manage legacy workloads and Kubernetes consoles to manage modern applications. This fragmentation slows down operations and complicates governance. This single-pane-of-glass approach for VMs alongside Kubernetes resources simplifies operations, ensures auditability, and applies consistent sovereign controls to legacy and cloud-native workloads, mirroring real-world architect workflows for scalable, secure multi-cluster solutions.

By the end of this exercise, you will:

. Navigate the RHACM Search interface to locate Virtual Machine resources across the fleet.
. Provision a new Virtual Machine using the RHACM Creation Wizard.
. Perform Day 2 operations (Stop, Start, Migrate) directly from the multi-cluster view.

=== Deploy a Virtual Machines Using the RHACM console

*Procedure*

[start=1]
. On the left switcher use the drop down to select *Fleet Virtualization*
+
image::08-dropdown-menu.png[link=self, window=blank, width=100%, Create VM]

. Click the *Create* dropdown select *From template*
+
image::09-create-menu.png[link=self, window=blank, width=100%, Create VM]

. Under default template select *Red Hat Enterprise Linux 9 VM*
+
image::10-template-menu.png[link=self, window=blank, width=100%, Create VM]
+
NOTE: Notice all of the avaiable templates to use, Windows VMs are fully supported as well.

. Under Virtual machine name, enter +rhel9-lab-vm+ and Leave all the options as Disk size, Disk Source.
+
[NOTE]
==== 
Please notice all of the available options to configure the VM with, these will change depending on your operating system
====
+
. Click *Quick create Virtual Machine* and watch for progress, this should take a couple of minutes to complete.
. Once completed you will see the live console as well as stats about your VM.

image::12-vm-finish.png[link=self, window=blank, width=100%, Create VM]

Feel free to explore the available management screens for VMs, from Metrics to Console to Snapshots and more! 

NOTE: *UI or GitOps—The Choice is Yours* While this lab focuses on the graphical capabilities of the RHACM console, remember that Virtual Machines in OpenShift are fundamentally Kubernetes objects.You can just as easily define your VMs as code (YAML) and deploy them using Red Hat OpenShift GitOps (ArgoCD). RHACM supports both workflows: use the UI for quick administration and discovery, or use GitOps for a fully declarative, audit-ready production pipeline.

[[container-workloads]]
=== Deploy and Manage Container Workloads via OpenShift GitOps

Red Hat® OpenShift® GitOps, powered by Argo CD and integrated with Red Hat Advanced Cluster Management (RHACM), makes application delivery faster, more secure, and consistent by using Git as the single source of truth to automatically handle declarative deployments and configurations across multiple clusters.

In sovereign cloud environments—where strict data residency, regulatory compliance, and operational control are essential—the optional pull model is ideal. Remote clusters securely pull updates from Git on their own, without needing inbound connections from a central hub, which strengthens security, minimizes network risks, and keeps workloads strictly within approved geographic or jurisdictional boundaries.

For this exercise, you’ll use the simpler push model (with Argo CD already deployed in your environment and ready for RHACM configuration), giving you a centralized, easy-to-manage approach that still demonstrates how GitOps delivers scalable operations while fully supporting sovereign cloud requirements in hybrid and multi-cloud setups.

=== Integrate ArgoCD with RHACM

*Procedure*

[start=1]
. From the **Fleet Management** tab, navigate to *Applications* from the left side menu.
. Click *Create application, select ArgoCD AppicationSet-Push Model*.
. Under the Argo server select *Add Argo Server* 
+
image::13-argo.png[link=self, window=blank, width=100%, ArgoCD Create]
+
. Enter the following information:
* *Name:* openshift-gitops
* *Namespace:* openshift-gitops
* *ClusterSet:* managed

image::03-argoconfig.png[link=self, window=blank, width=100%, ArgoCD Config]

[[ai-workloads]]
=== AI Workloads

AI and machine learning workloads require specialized compute resources and data locality considerations. RHACM enables you to deploy AI workloads across your cluster fleet while maintaining sovereignty requirements for data and compute resources.

[[workload-observability]]
=== Workload Observability

Workload observability provides insights into application performance, resource utilization, and health across your multi-cluster environment. This includes metrics, logs, and traces aggregated from applications running in different clusters and environments.

== Deploy Applications Using OpenShift GitOps

In this next step, you'll use Red Hat® OpenShift® GitOps with Argo CD to declaratively deploy the Skupper Patient Demo Application to both your local and hosted control plane (HCP) clusters—showcasing automated, Git-driven multi-cluster consistency while supporting sovereign cloud requirements by keeping patient data securely within approved geographic boundaries.

*Procedure*

. Navigate to *Applications* from the left side menu.
. Click *Create application*, select *ArgoCD AppicationSet-Push Model*.
. Enter the following information:
* *Name:* skupper-patient-demo
* *Argo Server:* openshift-gitops
. Click *NEXT*

image::03-app-gitops.png[link=self, window=blank, width=100%, App GitOps]

[start=5]
. Under repository types, select the GIT repository
. enter the URL:
+
[source]
----
https://github.com/mfosterrox/demo-applications.git
----
. Set Revision: main
. Set Path:
+
[source]
----
skupper-demo
----
. Set Destination:
+
[source]
----
patient-portal
----
. Then click *NEXT TWICE*

image::03-app-gitops-2.png[link=self, window=blank, width=100%, App ACM GitOps]

[start=11]
. Under *Sync Policy* uncheck *Automaticaly sync when cluster state changes* and check *Replace resources instead of applying changes from the source repository* 

image::03-app-gitops-3.png[link=self, window=blank, width=100%, App ACM GitOps]

NOTE: These changes are only required as you will be modifying the application YAML on RHACM and you don't want it to sync to a Git Repo, you normaly wouldn't uncheck these in a real production enviroment.

[start=12]
. Under *Placement* for application deployment, verify that *New Placement* is selected.
* Cluster set: managed
. Under *Label expressions* click *add label* and select the following
* *Label:* name
* *Operator:* equals any off
* *Values:* aws-us & local-cluster & hcp-emea

image::03-app-placement.png[link=self, window=blank, width=100%, ACM App Placement]

[start=14]
. Verify all of the information is correct and click *Submit*.

NOTE: It will take a few minutes to deploy the application

[start=15]
. Click on the *Topology Tab* to view and verify that *all of the circles are green*.

image::03-application-topology-git.png[link=self, window=blank, width=100%, Application Topology]

[start=16]
. Under the topology view, Select the *Route* and click on the *Launch Route URL*

NOTE: This will take you to the Front end for the Patient Portal application, which is now running in our Hosted Controlled Plane (HCP) CLuster.

IMPORTANT: If you get a "Application is not available" page change the URL to use http:// and that should fix the issue. 

image::03-application-route-git.png[link=self, window=blank, width=100%, Application Route]

Great work! 

You have successfully deployed an application using RHACM and OpenShift GitOps. This approach utilized a Git repository containing the manifests that defined your application. RHACM took those manifests and used them as deployables, which were then deployed to the target cluster.

[[cross-cloud-networking]]

== Cross-cloud Networking

Cross-cloud networking enables secure connectivity between workloads running in different clusters, cloud providers, or geographic regions. This capability is essential for sovereign cloud environments where data and applications must remain within specific jurisdictions while still enabling collaboration and data sharing across approved boundaries.

[[cross-cloud-migration]]

== Cross-cloud Migration

Cross-cloud migration capabilities allow you to move workloads between clusters and environments while maintaining data sovereignty and compliance requirements. This includes both planned migrations for optimization and emergency migrations for disaster recovery scenarios.

